{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59af67ec-db28-4f86-b9c7-a9a50a54c12d",
   "metadata": {},
   "source": [
    "# MAPseq Data Processing\n",
    "\n",
    "In this Jupyter notebook, we will guide you through the steps for turning raw sequencing reads into a viral barcode matrix, which you can then analyze yourself or in conjunction with BARseq data, as we will do in this course.\n",
    "\n",
    "All the code is Python, using standard libraries and idioms. But some of the key functionality is provided by the Pandas library, which itself is built on NumPy. To learn more about them, and the Python language visit: \n",
    "\n",
    "- https://www.python.org/\n",
    "- https://pandas.pydata.org/ \n",
    "- https://numpy.org/\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in python libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from configparser import ConfigParser\n",
    "\n",
    "# Data science libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe import from_pandas\n",
    "\n",
    "# For handling barcode tags or other fields with letters and numbers. \n",
    "from natsort import natsorted\n",
    "\n",
    "# Allowing to run our custom libraries from git area. \n",
    "gitpath=os.path.expanduser(\"~/git/mapseq-processing\")\n",
    "sys.path.append(gitpath)\n",
    "from mapseq.core import *\n",
    "from mapseq.barcode import *\n",
    "from mapseq.utils import *\n",
    "from mapseq.bowtie import *\n",
    "from mapseq.stats import *\n",
    "\n",
    "gitpath=os.path.expanduser(\"~/git/mapseq-analysis\")\n",
    "sys.path.append(gitpath)\n",
    "from msanalysis.analysis import * \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0882a-7ece-4209-b8d8-d3a48750d1e7",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The overall goal is to take the paired-end RNA sequencing FASTQ data and produce a correct matrix of viral barcodes and their projections to dissected areas. This is implemented as a pipeline of functions, with the first consuming the FASTQs and the others consuming the output of the previous step, until the final step produces the matrices.  \n",
    "\n",
    "![title](files/mapseq-processing-schema-api.png)\n",
    "\n",
    "# Configuration, logging, and paths for analysis\n",
    "We need to provide paths to the two fastq read files (either fastq or fastq.gz), the path to the a standard Python configuration file, and a standard sample information Excel spreadsheet. We also have a convenience method to package up the FASTQ file pairs.  \n",
    "\n",
    "This is an example sample info spreadsheet. \n",
    "\n",
    "![title](files/sampleinfo-xlsx.png)\n",
    "\n",
    "The Site information column should indicate whether the dissected area is a target or injection, and additionally whether it is a negative (biological control) or control (water). \n",
    "Region and Brain should be obvious. Note that the region labels will be used (if available) for the default plots.  \n",
    "\n",
    "The RT primer barcode table is included in the software, as \n",
    "\n",
    "<code>~/git/mapseq-processing/etc/barcodes_v2.txt</code>\n",
    "\n",
    "By default, functions in the pipeline take their parameters from a single configuration file, included in the distribution <code>~/git/mapseq-processing/etc/mapseq.conf</code>. If you want to perform processing with parameters other than the defaults, then copy the config in the distrubtion, edit it, and use it instead. For some functions, parameters can be overridden in the function call itself.  \n",
    "\n",
    "Here are a couple example sections of the configuration file. We will revisit it for each function in more detail. \n",
    "\n",
    "![title](img/mapseq-config.png)\n",
    "\n",
    "A configuration file simply has sections, within which there are key-value pairs. These values can be retrieved by the ConfigParser interface.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp = get_default_config()\n",
    "configfile = os.path.expanduser('~/mapseq/M205/M205.mapseq.conf')\n",
    "cp = ConfigParser()\n",
    "cp.read(configfile)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "sampleinfo = os.path.expanduser('~/mapseq/M205/M205.sampleinfo.xlsx')\n",
    "bcfile = os.path.expanduser( cp.get('barcodes','ssifile') )\n",
    "project_id = cp.get('project','project_id')\n",
    "\n",
    "infilelist = [\n",
    "    os.path.expanduser('~/mapseq/M205/fastq/M205_HZ_S1_R1_001.fastq.gz'),\n",
    "    os.path.expanduser('~/mapseq/M205/fastq/M205_HZ_S1_R2_001.fastq.gz')\n",
    "          ]\n",
    "infiles = package_pairs(infilelist)\n",
    "outdir = os.path.expanduser('~/mapseq/M205')\n",
    "print(f\"For {project_id}:\\nconfig={configfile}\\nbcfile={bcfile}\\ninfiles={infiles}\\noutdir={outdir}\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d15d163-bed1-4a26-a537-230469c29d10",
   "metadata": {},
   "source": [
    "# Load Sample Information\n",
    "\n",
    "We load the information from the Excel spreadsheet into a Pandas dataframe. What you see should correspond to what was in the Excel file. \n",
    "If it does not, check the formatting of the fields in the spreadsheet (leading/trailing spaces, incorrect capitalization, etc.). The loading code expects the column headers to be exactly as shown here, no blank rows, and no extraneous info below the main table.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampdf = load_sample_info(sampleinfo, cp=cp)\n",
    "sampdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ce781-5895-4d51-a937-d3c3c12490d4",
   "metadata": {},
   "source": [
    "# process_fastq_pairs()\n",
    "\n",
    "Here is an example of the first few lines of a FASTQ-formatted data file. \n",
    "\n",
    "![title](files/fastq-format.png)\n",
    "\n",
    "Information about each read uses four lines of the file. The first line is the sequence ID, often containing info about the experiment, sequencing hardware, and run-specific metadata.  \n",
    "The second line is the actual sequence. \n",
    "The third line may have optional information. \n",
    "The fourth line has encoded quality estimates for each of the bases in line two.  \n",
    "\n",
    "We are only interested in the sequence line. \n",
    "\n",
    "Note that some raw data may come in multiple pairs of files, and is often compressed (with gzip). The program can read multiple pairs as long as they sort properly alphabetically, and it correctly handles compressed or uncompressed files. This function pulls out the sequence lines from the first read FASTQ, and joins them with the corresponding sequence from the seconds read FASTQ.\n",
    "\n",
    "The function pulls out all the sequence lines, splits the sequence by fields, and generates a read-oriented table in a form that is easy to manipulate. Each row of the dataframe is a single read. \n",
    "\n",
    "![title](files/fastq-schema.png)\n",
    "\n",
    "Run the cell. It will call the function and display the resulting table. For our data this should take about a minute. As it runs, it will display progress messages per 1000000 sequences.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d340e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle all the input. usually takes ~2 minutes\n",
    "# M205.htna25\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "rdf = process_fastq_pairs(infiles, outdir=outdir, cp=cp)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905d408-e6c4-4ec4-9862-157039886931",
   "metadata": {},
   "source": [
    "Note that the number of rows represents the total number of raw reads. At this point we have retained the initial 52nt full read.\n",
    "Below is additional Pandas code that shows examples of how further information can be extracted at this point. Feel free to edit and re-run. Whatever the last variable or call is will be displayed.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6a673-eeff-4afc-bc58-7024afef66c5",
   "metadata": {},
   "source": [
    "### aggregate_reads_dd()\n",
    "Each row represents a read, but many of these rows are identical (or they should be, if the sequencing read depth is sufficient). \n",
    "So we aggregate the reads and set a read_count column. This greatly reduces the dataframe size. We are using Dask under the hood to do this, because most real datasets will exceed system memory.  \n",
    "\n",
    "Dask requires a dataframe in its own format, so we convert it. \n",
    "\n",
    "This step also drops reads with only a single copy. If you want to retain all, set min_reads=1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be862bb1-fc63-4865-94af-3a52aed30099",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = aggregate_reads(rdf, outdir=outdir, cp=cp)\n",
    "adf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06199e-8e6a-4810-8e39-076b28a9e626",
   "metadata": {},
   "source": [
    "Note how the row count has dropped signficantly--from 17M to 4M. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309427d-65ea-4dfe-ae6b-77439b633a71",
   "metadata": {},
   "source": [
    "# split_fields()  and filter_fields()\n",
    "\n",
    "Now we are going to filter the reads in order to remove rows that have 'N' characters, which are nucleotides the sequencing process was unable to resolve. We will also remove sequences with long runs of the same nucleotide (>7) since we know these result in unreliable sequences. \n",
    "\n",
    "We will also now split the overall sequence into MAPseq-specific fields, and drop the full sequence column (since we don't need it anymore) \n",
    "\n",
    "This function should only take a few seconds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59e889-cd27-4af8-955c-539df84b8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = split_fields(adf.copy(), column='sequence', drop=True, cp=cp )\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd57b2-217c-4fce-bf82-308d35c510d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = filter_fields(sdf.copy(), cp=cp)\n",
    "fdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef761c5-130b-4734-bbc1-992e4542ed9b",
   "metadata": {},
   "source": [
    "Below is an area to again play with data exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165b0a3-e836-4e8b-9e95-762e1577bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare libtag diversity after filtering with that above. \n",
    "fdf['libtag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7dcf6-a361-4548-9658-803c8f5cd9ed",
   "metadata": {},
   "source": [
    "This is a good time to be aware that many of the functions in the pipeline fill in statistics and QC information in a file named <code>stats.\\<datestring\\>.json</code> where the datestring is the time the Jupyter session was initialized. These stats files are intended to be both human and machine readable, so they can be used to make reports, or plots. \n",
    "\n",
    "Here is an example of the contents of the stats file at this point of the processing:\n",
    "\n",
    "![title](files/stats-fastq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116d045-9cad-486a-b7fa-f3cfaaabc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['read_count'].max()\n",
    "fdf['read_count'].median()\n",
    "fdf['read_count'].describe()\n",
    "fdf['vbc_read'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af23ff-0990-4c0a-9bcd-80dc1bbf3bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3b0a56d-17e1-43b8-a187-383ee08b6570",
   "metadata": {},
   "source": [
    "# process_make_readtable_pd()\n",
    "\n",
    "Now that we have the fields of interest, we can fill in, for convenience, the information determined by the SSI barcode, identify the libtags, and identify spike-in sequences. As part of that process, we will also identify reads which have errors such that that information cannot be filled in, and remove them. \n",
    "\n",
    "This is where we calculate an estimate of the template switching rate. The evidence for this is when we see an L1 libtag in a read with a valid target SSI. Information needed to calculate the rate is in the stats entry for the readtable function.\n",
    "\n",
    "As part of this we will automatically generate read count frequency plots so that we can confirm that our read count threshold (2 by default) are reasonable. Frequency plots are done separately for target and injection, since those are sequenced in different pools. In the raw plot, we include an **estimated_threshold**. This is a value calculated as the threshold that retains 90% of the total data.  \n",
    "\n",
    "This step should only take a few seconds...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdf = process_make_readtable_pd( fdf.copy(),\n",
    "                                   sampdf,\n",
    "                                   bcfile=bcfile, \n",
    "                                   outdir=outdir, \n",
    "                                   cp=cp)\n",
    "rtdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15e17c-51c5-481b-b2c3-fd6372b5c58e",
   "metadata": {},
   "source": [
    "Note that data from the sample information spreadsheet has now been integrated into the read table. The type value has been set from the libtag and the spike sequence. The label, rtprimer number, and brain id has been set from matching the SSI sequence. \n",
    "\n",
    "Since this is a small dataset, and read depth is thus low, a read threshold of 2 is reasonable. \n",
    "\n",
    "Below is a cell to play with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f93f1-cadd-4802-b80b-68e39d6c023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get an idea of the number of reals and spikes by target area\n",
    "rtdf.groupby(['label','type'], observed=True ).agg( { 'umi':'nunique', 'read_count':'sum'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf9b4d-7aa5-4a8a-9e2e-47d8e1eac974",
   "metadata": {},
   "source": [
    "# align_collapse()\n",
    "\n",
    "As the MAPseq virus replicates in the cell, it is prone to a fairly high error rate. If we didn't account for this, we might attribute viral barcodes that came from a single cell to multiple cells. So we are going to find all the sets of viral barcodes (vbc_read) column that are within a Hamming distance of 3 (i.e. all sequences that can be made identical with 3 or less single nucleotide edits). For each of those sets, we will set the vbc_read sequence to the same sequence. This schematic illustrates the process.  \n",
    "\n",
    "![title](files/align-collapse.png)\n",
    "\n",
    "Note that the number of rows has not changed, and each row still represents one read. Even after collapse we retain the read_count for the rows, so at any point we can threshold on that value. \n",
    "\n",
    "We will perform the search by doing an all x all alignment using bowtie. We parse the bowtie output to create an edge graph. We use Tarjan's algorithm to find all the components for a given graph. Then we collapse the vbc_read sequences to the most common sequence variant in the component.  \n",
    "\n",
    "This is the most memory and CPU-intensive function in the pipeline, and can take a long time for large datasets. For the workshop the dataset is relatively small, so it should complete in 3-5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c17c16-9fe6-492b-b2ee-cec0727c88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = align_collapse_pd(rtdf.copy(), \n",
    "                       outdir=outdir, \n",
    "                       cp=cp)\n",
    "cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb908f-c81e-4011-93bb-33260a43da82",
   "metadata": {},
   "source": [
    "Again, note that the total number of rows is the same. The vbc_read column now contains the \"fixed\" sequence. \n",
    "\n",
    "We can look at the new entry in the stats file for deeper insight. \n",
    "\n",
    "![title](files/stats-collapse.png)\n",
    "\n",
    "We can see that of ~16M VBC sequences, there were only 715k unique vbc_read sequences. Component numbers can also be informative. n_components is the total number of Hamming sets. n_multi_components is the number of those sets that had more than one distinct sequence in them (meaning there was a mutation). So the large majority of Hamming components consist of sequence variants.  \n",
    "\n",
    "The cell below is for exploration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158dcabc-5eae-405d-a22e-f1bfce674d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare diversity of read values after collapse to the one above before colllapse\n",
    "# Why did the mean go down?\n",
    "cdf['vbc_read'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd92dd-b516-4d04-8dd4-4dba4ef703e7",
   "metadata": {},
   "source": [
    "# process_make_vbctable_pd()\n",
    "\n",
    "So far the data tables have been read-oriented (each row is a read). Now we are ready to move toward a viral barcode-oriented format, with each row representing a viral barcode, which represents a single neuron. And this is the step where we will threshold on the read_count value we checked above. \n",
    "\n",
    "After data thresholding and cleaning, and dropping some redundant columns, this function aggregates on the viral barcode and sums the UMI count. This function takes less than a minute.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf388f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf = process_make_vbctable_pd(cdf.copy(),\n",
    "                               outdir=outdir,\n",
    "                               cp=cp)\n",
    "vdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b42a1-7f9c-45af-9d7e-d1affd1f898f",
   "metadata": {},
   "source": [
    "Note that the number of rows has now dropped from ~16M to ~350K, and we have per-barcode umi_count and (aggregate) read_count for each UMI. Recall that the vbc_read_col column has duplicate entries, but they should differ in other columns. \n",
    "\n",
    "Below is a cell to explore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e1515-2f37-46ed-8de9-5326ce68ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bae3c8-802d-4563-97a0-893fe9d9a147",
   "metadata": {},
   "source": [
    "# process_filter_vbctable()\n",
    "\n",
    "We now have the full viral barcode table, with UMI counts for real and spike-in sequences.\n",
    "We previously thresholded the data for read_counts, and removed reads with invalid or missing values. This function prepares the data so that the matrix generation code can be as simple as possible. So here we will establish minimum UMI counts for inclusion in the final matrices. \n",
    "\n",
    "This experiment does not have injection barcodes from MAPseq (they will come from BARseq). If it did, this is where we would only include barcodes that appear in both an injection area and at least one target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0023c-ecac-4226-8d1b-52891d289e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfdf = process_filter_vbctable( vdf, \n",
    "                                target_min_umi = 2, \n",
    "                                outdir=outdir,\n",
    "                                cp=cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2e53a-7098-4389-9fa0-941d1fa7bea2",
   "metadata": {},
   "source": [
    "# process_make_matrices_pd()\n",
    "\n",
    "For each brain, we pivot the real VBCs against the SSI/labelled site to create a 2D matrix, with the value being the UMI count for that VBC in that site.\n",
    "We do the same operation for real VBCs filtered by the thresholds and requirements as defined in the config file. \n",
    "We do the same operation for the spike-in barcodes. \n",
    "Within each target area, we then normalize the filtered real UMI count by the total spike-in UMI count for that area: \n",
    "\n",
    "![title](files/normalization.png)\n",
    "\n",
    "The results are pre-brain normalized barcode matrices, labelled by brain. E.g. YW144.nbcm.tsv\n",
    "\n",
    "The file names as follows:\n",
    "- rbcm   = raw real barcode matrix\n",
    "- fbcmdf = filtered (real) barcode matrix\n",
    "- sbcm   = spike-in matrix\n",
    "- nbcm   = normalized barcode matrix\n",
    "- scbcm  = scaled normalized barcode matrix. \n",
    "\n",
    "The scale normalized matrix forces all values to 0.0 - 1.0 for usage in heatmap plots that require scaled values. \n",
    "\n",
    "This function should run in less than 30 seconds. Since this function doesn't produce a single dataframe, we print a less-pretty version of the normalized matrices. All the matrices are saved as TSV files in the out directory.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f636a-4d8d-4211-bd7e-a853d7c0b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "(real_dict, spike_dict, norm_dict) = process_make_matrices(vdf,\n",
    "                                                           sampdf=sampdf,  \n",
    "                                                           outdir=outdir,\n",
    "                                                           cp=cp)\n",
    "for bid in norm_dict.keys():\n",
    "    print(f'brain={bid}:\\n{norm_dict[bid]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6891b07-2685-45d5-af35-d6d3d1860cbf",
   "metadata": {},
   "source": [
    "As matrices, this data is a bit harder to infer anything by eye, but is in a form useful for plotting. \n",
    "\n",
    "Below is a cell for exploration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f8a58-c2d3-49ca-861b-3d8a84199fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbcm = norm_dict['YW143']\n",
    "nbcm\n",
    "nbcm['BC5'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc5119-0f17-42ef-872d-771194270466",
   "metadata": {},
   "source": [
    "# Output files\n",
    "\n",
    "Much more data is produced than is reflected in this notebook. To further investigate you can examine the output directory to open those files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf359007-9aed-4892-a785-87f25782795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = os.listdir(outdir)\n",
    "flist.sort()\n",
    "flist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204191c-b963-46de-9fbe-d27e34626807",
   "metadata": {},
   "source": [
    "# Further data analysis\n",
    "\n",
    "For simple data validation, we find it useful to create binarized plots, organized hierarchically. \n",
    "The y-axis is all the viral barcodes, while the x-axis is the target areas. For each barcode, the target area is given color if any UMIs are present. We have hierarchically arranged the VBCs such that all VBCs that project to any combination of target areas are grouped together. The vertical height of the color block in any target area reflects the number of projections. And blocks arraned horizontally in line with one another represent neurons with projections to multiple target areas. This allows the identification of projection motifs.\n",
    "\n",
    "Since in this case we have two brains with the same target areas, obvious patterns across them are likely to reflect biological reality. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7e994-36a3-4b32-9e6f-939ae8ffc283",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f'{outdir}/YW143.nbcm.tsv'\n",
    "nbmdf = load_mapseq_df(infile)\n",
    "nbmdf\n",
    "g = get_plot_binarized(nbmdf, expid=f'{project_id}:YW143') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3a5cb-5b8f-40f7-a227-e0146a8d93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f'{outdir}/YW144.nbcm.tsv'\n",
    "nbmdf2 = load_mapseq_df(infile)\n",
    "nbmdf2\n",
    "g = get_plot_binarized(nbmdf2, expid=f'{project_id}:YW144') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6983460-a856-4dfc-867a-093732c8d5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
